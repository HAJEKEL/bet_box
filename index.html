DOCTYPE HTML>
<!--
	Massively by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>

<head>
    <title>Computer vision by deeplearning</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
    <link rel="stylesheet" href="assets/css/main.css" />
    <noscript>
        <link rel="stylesheet" href="assets/css/noscript.css" />
    </noscript>

</head>

<body class="is-preload">

    <!-- Wrapper -->
    <div id="wrapper" class="fade-in">

        <!-- Intro -->
        <div id="intro">
            <h1>Computer vision by deeplearning </h1>
            <h2>The Lottery Tickets Hypothesis for Supervised
                Pre-training in depth estimation models.</h2>
            <p>
                <a href="https://www.linkedin.com/in/henk-jekel-748054259/" target="_blank">
                    <img src="images/linkedin_icon.png" alt="LinkedIn Pictogram" width="35" height="35">
                </a>
                <a href="https://github.com/HAJEKEL/pt.darts" target="_blank">
                    <img src="images/github_icon.png" alt="Github Pictogram" width="35" height="35">
                </a>
                <a href="mailto:hendrikjekel@gmail.com" target="_blank">
                    <img src="images/gmail_icon.png" alt="Gmail Pictogram" width="35" height="35">
                </a>
            </p>
            <ul class="actions">
                <li>
                    <h4>Keep reading to learn more</h4>
                    <a href="#nav" class="button icon solid solo fa-arrow-down scrolly">Continue</a>
                </li>
            </ul>
        </div>


        <!-- Nav -->
        <nav id="nav">
            <ul class="links">
                <li class="active">
                    <a href="https://hajekel.github.io/">
                        <span style="font-size: 35px;">&larr;</span> Back to portfolio
                    </a>
                </li>
            </ul>




            <ul class="icons">
                <li><a href="https://www.linkedin.com/in/henk-jekel-748054259/"
                        class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a>
                </li>
                <li><a href="https://www.kaggle.com/hajekel" class="icon brands fa-kaggle"><span
                            class="label">Kaggle</span></a></li>
                <li><a href="https://github.com/HAJEKEL" class="icon brands alt fa-github"><span
                            class="label">GitHub</span></a></li>
            </ul>
        </nav>

        <!-- Main -->
        <div id="main">

            <!-- Featured Post -->
            <article class="post featured">
                <header class="major">
                    <span class="date">April 28, 2023</span>
                    <h1>Computer vision by deeplearning </h1>
                </header>
                <h2>The Lottery Tickets Hypothesis for Supervised
                    Pre-training in depth estimation models. </h2>
                <!-- 
                <a class="image fit"><img src="images/EP50-reduce.png" alt="EP50-reduce" /></a>
                -->
                <h2>
                    Abstract
                </h2>
                <p>
                    Abstract:

                    In the field of computer vision, pre-trained models have gained renewed attention, including
                    ImageNet supervised pre-training. Recent studies have highlighted the enduring
                    significance of the Lottery Tickets Hypothesis (LTH) in the context of classification, detection,
                    and segmentation tasks. Inspired by this, we set out to explore the potential of LTH in the
                    pre-training paradigm of depth estimation. Our aim is to investigate whether we can significantly
                    reduce the complexity of pre-trained models without compromising their downstream transferability in
                    the depth estimation task. We fine-tune the sparse pre-trained networks obtained through iterative
                    magnitude pruning and demonstrate universal transferability to the depth estimation task,
                    maintaining performance comparable to that of fine tuning on the full pre-trained model. Our
                    findings are inconclusive.

                </p>
                <h2>
                    Introduction
                </h2>
                <p>
                    In the realm of computer vision, the resurgence of interest in pre-trained models, including
                    classical ImageNet supervised pre-training, has sparked enthusiasm. Recent studies suggest that the
                    core observations of the Lottery Tickets Hypothesis (LTH) remain relevant in the pre-training
                    paradigm of classification, detection, and segmentation tasks (Chen et al., 2021) as displayed in
                    the following figure.

                <figure>
                    <a class="image fit">
                        <img src="images/transfer_learning_LTH.jpg" alt="Description of the image" />
                    </a>
                    <figcaption><span style="font-size: smaller; font-weight: bold;">Studies show that
                            within pre-trained computer vision models (both supervised and
                            self-supervised), there are matching subnetworks that exhibit transferability to multiple
                            downstream tasks, with minimal performance degradation compared to using the full
                            pre-trained
                            weights. Task-agnostic and universally transferable subnetworks are found during pre-trained
                            initialization, benefiting classification, detection, and segmentation tasks.(Chen et al.,
                            2021) </span>
                    </figcaption>
                </figure>


                Driven by curiosity, we pose the following question:
                </p>
                <h3>
                    Can we aggressively trim down the complexity of
                    pre-trained models without compromising their downstream transferability on the depth estimation
                    task?
                </h3>

                <p>

                <figure>
                    <a class="image fit">
                        <img src="images/transfer_learning_LTH_depth.png" alt="Description of the image" />
                    </a>
                    <figcaption><span style="font-size: smaller; font-weight: bold;">We wonder if the sparse
                            subnetworks also transfer to the depth estimation task.</span>
                    </figcaption>
                </figure>

                In this engaging blog post, we explore the concept of supervised pre-trained models through the lens
                of the Lottery Tickets Hypothesis (LTH) (Frankle & Carbin, 2019). LTH identifies highly sparse
                matching subnetworks that can be trained almost from scratch and still achieve comparable
                performance to the full models. Extending the scope of LTH, we investigate whether such matching
                subnetworks exist in pre-trained computer vision models, specifically examining their transfer
                performance in the context of depth estimation.

                Our experiments did not confirm nor deny the possibility of the hypothesis as the experiments failed.
                The codes and pre-trained models used in our experiments can be accessed on
                GitHub: <a https://github.com/HAJEKEL/CV_LTH_pre-training_depth-estimation >github.com/HAJEKEL/CV_LTH_pre-training_depth-estimation</a>.

                Join us on this fascinating journey as we uncover the untapped potential of pre-trained models and
                shed light on the intriguing relationship between the Lottery Tickets Hypothesis and depth
                estimation in computer vision.
                </p>

                <h2>
                    Literature research
                </h2>

                <h3> Lottery ticket hypothesis</h3>

                <p>
                    During his PhD research, Jonathan Frankle conducted a thorough investigation into the lottery ticket
                    hypothesis. Initially, the hypothesis proposed that it was feasible to identify sparse subnetworks
                    within dense networks that would perform comparably to the dense network after training. This
                    approach relied on iterative magnitude pruning, where the network was trained until convergence.
                    Subsequently, all weights below a certain magnitude threshold were pruned, and the remaining weights
                    were reset to their initial random values. This process was repeated until the desired sparsity
                    level was achieved, while still preserving the performance of the dense model as displayed in the
                    following gif.
                <figure>
                    <a class="image fit">
                        <img src="images/LTH.gif" alt="Description of the image" />
                    </a>
                    <figcaption><span style="font-size: smaller; font-weight: bold;">An animation that shows the process
                            of iterative magnitude pruning proposed in the paper "The Lottery Ticket Hypothesis: Finding
                            Sparse, Trainable Neural Networks" (J. Frankle & M. Carbin, 2019)</span>
                    </figcaption>
                </figure>


                However, it was later revealed that this hypothesis did not hold for larger models. As a result,
                Frankle put forth a modified hypothesis that was applicable to larger models. Instead of resetting
                the weights to their original random initialization, he proposed resetting them to an earlier point
                in the training process. This breakthrough led to the publication of the influential paper "Linear
                Mode Connectivity and the Lottery Ticket Hypothesis," which demonstrated that regardless of the
                stochastic gradient descent (SGD) noise, models would converge to the same linearly connected
                minimum when initialized at an early training point instead of their random initialization as
                displayed in the following image.

                <figure>
                    <a class="image fit">
                        <img src="images/linear_mode_connectivity.jpg" alt="Description of the image" />
                    </a>
                    <figcaption><span style="font-size: smaller; font-weight: bold;">A diagram of instability analysis
                            from step training step 0 (left) and
                            step k (right) when comparing networks using linear interpolation. At a certain training
                            step k the subnetwork becomes stable to SGD noise. The weight values at train step k are the
                            weight values that need to be used when resetting the weights in iterative magnitude pruning
                            for uptaining the sparse subnetworks. </span>
                    </figcaption>
                </figure>

                Building upon these seminal findings, our study focuses on applying this second approach in the
                context of a large ResNet-50 model trained on ImageNet. We employ iterative magnitude pruning, where
                the remaining weights are iteratively reset to an early point in training. Specifically, we
                investigate the application of this resetting method in iterative magnitude pruning for pre-trained
                classification models on ImageNet. Furthermore, we utilize the obtained weight mask to validate that
                the downstream transferability for monocular depth estimation is not compromised.

                Depth estimation is a crucial step towards inferring scene geometry from 2D images. The goal in
                monocular depth estimation is to predict the depth value of each pixel or inferring depth information,
                given only a single RGB image as input. This is generally done using an encoder decoder network.

                By delving into the implications of the modified lottery ticket hypothesis and leveraging the
                iterative magnitude pruning technique on a pre-trained model, our research aims to provide
                compelling evidence supporting the preservation of downstream transferability in the realm of depth
                estimation.


                </p>

                <h2>
                    Methods
                </h2>
                <p>
                    This section descripes the experiments done. First the models used are described after which the
                    exact
                    mask used will be described. The final part of this section describes all specifics about the
                    training procedure.
                </p>
                <h3>
                    Model
                </h3>
                <p>
                    For the experiments the FCRN model architecture will be used, this model architecture was proposed
                    by Laina et al. (2016),
                    the model uses ResNet-50 as a base and a self-designed upsampling end. Instead of the fully
                    connected end layer,reinforce the relevance of LTH in the pre-training paradigm of depth estimation,
                    paving the
                    way for more efficient and effective depth estimation models.
                    which is used for pre-training, the model has upsampling blocks to generate the 2D depth estimation
                    output.
                </p>
                <figure>
                    <img src="images/model_architecture.png" alt="Model" style="width:100%">
                    <figcaption>Fig.1 - Complete model architecture including the base ResNet-50 model (first two lines)
                        and the new upsampling blocks (Laina et al.,2016).</figcaption>
                </figure>
                <h4>
                    Masking
                </h4>
                As described above, we want to combine the proposed model, with the masking of the lottery hypothesis
                paper.
                To do this we mask the model using a mask provided by Chen et al. (2021). The mask used had a sparsity
                of 1.88%
                <h3>
                    Dataset
                </h3>
                <p>
                    For training the model we used the depth dataset of NYU, it consists of 1449 frames of rgb-Depth
                    pictures.
                    We only use the labeled part of the dataset and not the unlabeled part which is even bigger.
                    For more information about the dataset look at this page: <a
                        href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html"> nyu dataset</a>
                </p>
                <h3>
                    Training procedure
                </h3>
                <p>
                    The models were trained on the labeled section of the NYU depth dataset,
                    we trained till convergence for both the sparse and dense model. For the loss function we took the
                    berHu loss which was used by
                    Laina et al. (2016) and design by (L. Zwald and S. Lambert-Lacroix., 2012).
                </p>
                <h4>
                    loss function
                </h4>
                <p>
                    For training we created our own pipeline to connect the dataset and the model. The berHu loss
                    function was used to train the model, it is a loss function that combines the L1 and L2 losses based
                    on the highest pixel error in
                    each minibatch (L. Zwald and S. Lambert-Lacroix., 2012). This choice was made because it was used by
                    Laina et al. (2016) on the same task.

                </p>

                <h4>
                    optimizer
                </h4>
                <p>
                    The standard torch SGD optimizer was used to change the values based on the gradients coming from
                    the loss function.
                    We used a learning rate of 0.0001 for the Resnet-50 part of the model and a learning rate of 0.002
                    (x20) for the upsampling blocks.
                    The momentum parameter (&beta;) was set to 0.9 and weight decay was set to 0.01. These setting were
                    taken from the code of Laina et al. (2016) except for the learning rate, which originally was 0.001.
                    The learning rate was decreased, because the model would diverge exponentially in the first epochs
                </p>
                <p>
                    As one of the compared models is masked, we need to mask the gradients of this model as well. This
                    is done using the same mask used for masking the model itself.
                </p>

                <h2>
                    Results
                </h2>
                <p>
                    Below are some depth maps generated by the models, as you can see

                <figure>
                    <img src="images/output_1.png" alt="Model" style="width:100%">
                    <figcaption>Fig.2 - Depth maps generated by the models next to in put and the groundtruth depthmap.
                    </figcaption>
                </figure>

                <figure>
                    <img src="images/output_2.png" alt="Model" style="width:100%">
                    <figcaption>Fig.3 - Depth maps generated by the models next to in put and the groundtruth depthmap.
                    </figcaption>
                </figure>
                To answer our research question : <i>Can we aggressively trim down the complexity of
                    pre-trained models without compromising their downstream transferability on the depth estimation
                    task?</i>, we evaluated multiple criteria presented in previous papers. The values are the relative
                distance(rel), relative mean squared error (rms) and its log. The final three columns represent
                the &#948;<sub>123</sub> , which are measure that state how much pixels are correctly esitmated
                for a given error threshold.
                <table>
                    <tr>
                        <th>Model</th>
                        <th>rel</th>
                        <th>rms</th>
                        <th>δ<sub>1</sub></th>
                        <th>δ<sub>2</sub></th>
                        <th>δ<sub>3</sub></th>
                    </tr>
                    <tr>
                        <td>FCRN_dense (Laina et al.(2016))</td>
                        <td>0.127</td>
                        <td>0.573</td>
                        <td>0.811</td>
                        <td>0.953</td>
                        <td>0.988</td>
                    </tr>
                    <tr>
                        <td>FCRN_dense(OURS)</td>
                        <td>0.325</td>
                        <td>1.239</td>
                        <td>0.428</td>
                        <td>0.740</td>
                        <td>0.904</td>
                    </tr>
                    <tr>
                        <td>FCRN_sparse(OURS)</td>
                        <td>0.339</td>
                        <td>1.407</td>
                        <td>0.406</td>
                        <td>0.699</td>
                        <td>0.886</td>
                    </tr>
                </table>
                Unfortunately The resutls show that our model does not work as well as the model of Laina et al.,
                for this there are multiple possible reasons.
                <ol>
                    <li>
                        The model we compare with used not only the label NYU dataset,
                        but also trained on different data, resulting in better generalization.
                    </li>
                    <li>
                        The training pipe was self designed, this results in the posibillity for bugs.
                        As can be seen in the graph, the difference between our two models
                        is not that big but there is a big difference with regard to the other model.
                    </li>
                </ol>

                From the results we can see that the sparse model does not behave alot worse compared
                to our dense trained network, but due to the big gap in performance compared to the models
                of others we cannot give a real conclusive answer to the question.

                </p>
                <h2>
                    References
                </h2>
                <p>
                <ol>
                    <li>
                        Silberman, N., Hoiem, D., Kohli, P., & Fergus, R. (2012). Indoor Segmentation
                        and Support Inference from RGBD Images. In Lecture Notes in Computer Science
                        (pp. 746–760).
                        Springer Science+Business Media. <a href="https://doi.org/10.1007/978-3-642-33715-4_54">
                            https://doi.org/10.1007/978-3-642-33715-4_54</a>

                    </li>

                    <li>Laina, I., Rupprecht, C., Belagiannis, V., Tombari, F., & Navab, N. (2016).
                        Deeper Depth Prediction with Fully Convolutional Residual Networks.<a
                            href="https://doi.org/10.1109/3dv.2016.32">https://doi.org/10.1109/3dv.2016.32</a>

                    </li>
                    <li>
                        L. Zwald and S. Lambert-Lacroix. The berhu penalty and the
                        grouped effect. <a href="arXiv preprint arXiv:1207.6868, 2012. 2, 4,">arXiv preprint
                            arXiv:1207.6868, 2012. 2, 4,</a>
                        5
                    </li>

                    <li>
                        Frankle, J., & Carbin, M. (2019). The Lottery Tickets Hypothesis for Supervised and
                        Self-supervised Pre-training in Computer Vision Models.
                    </li>
                    <li>
                        Chen, T., Frankle, J., Chang, S., Liu, S., Zhang, Y., Carbin, M., & Wang, Z. (2021). The Lottery
                        Tickets Hypothesis for Supervised and Self-supervised Pre-training in Computer Vision Models.

                    </li>

                </ol>
                </p>
                <!-- 

                <p>
                    DARTS (Differentiable Architecture Search) is a method for automating the search for neural
                    network architectures. The goal is to find the best architecture for a given task without
                    requiring human expertise in designing neural networks.

                    In the search stage, the DARTS algorithm uses a differentiable relaxation of the architecture
                    search space to learn the best architecture. This involves learning the weights of the different
                    network operations in the architecture. The operations out of which DARTS could choose where
                    DSConv,
                    MBConv and Fused-MBConv.

                    The DARTS search was based on the Fashion-MNIST dataset. The Fashion-MNIST dataset is a popular
                    benchmark dataset used in machine learning and computer vision research. It consists of 70,000
                    grayscale images of 28x28 pixels each, divided into 10 classes, with 7,000 images per class. The
                    classes include T-shirts/tops, trousers, pullovers, dresses, coats, sandals, shirts, sneakers,
                    bags, and ankle boots. The dataset is a more challenging alternative to the classic MNIST
                    dataset, as it features more complex images with greater variability in the appearance of the
                    different classes.

                    It is worth noting that the distribution of the classes is roughly balanced, with each class
                    accounting for 10% of the dataset. This makes the dataset suitable for evaluating the
                    performance of machine learning algorithms in a multi-class classification setting.

                    In the context of DARTS (Differentiable Architecture Search), the terms "normal" and "reduce"
                    are used to refer to two distinct types of cells that are utilized in the process of
                    architecture search.

                    A "normal" cell is defined as a cell that maintains an unchanged spatial resolution between the
                    input and output, whereas a "reduce" cell is a cell that reduces the spatial resolution between
                    the input and output.

                    More details on DARTS in the downloadable pdf:

                </p>
                <ul class="actions special">
                    <li><a href="DARTS.pdf" class="button large">Download PDF</a></li>
                </ul>

                <h2>
                    Results
                </h2>

                <p>
                    The experiments conducted involved training DARTS on the FashionMNIST dataset for 50 epochs,
                    utilizing the only available operations DSConv, MBConv, and Fused-mbconv. Two visualizations of
                    the training process were presented, which included a gif that displayed the progress of each
                    epoch for both the normal and reduce cells. Additionally, an image was provided that contained
                    two plots demonstrating the progress of the training: the first plot indicated the progression
                    of the loss throughout the training process, while the second plot showed the corresponding
                    accuracy progression. These visualizations and plots were indicative of the effectiveness and
                    efficiency of the training process, and demonstrated the potential of DARTS to be employed in
                    real-world tasks.





                    <a class="image fit"><img src="images/normal.gif" alt="normal" /></a>
                    <a class="image fit"><img src="images/reduce.gif" alt="reduce" /></a>
                    <a class="image fit"><img src="images/loss_accuracy.png" alt="loss_accuracy" /></a>

                    The training log can be found on github:
                <ul class="actions special">
                    <li><a href="https://github.com/HAJEKEL/pt.darts/blob/master/searchs/fashionmnist/fashionmnist.log"
                            class="button large">Visit training log</a></li>
                </ul>


                </p>

                <h2>
                    Conclusion
                </h2>


                <p>
                </p>

                <h2>
                    Code implementation
                </h2>

                <p>
                    Code for this blog:
                <ul class="actions special">
                    <li><a href="https://github.com/HAJEKEL/CV_LTH_pre-training_depth-estimation/tree/website"
                            class="button large">WEBSITE</a></li>
                </ul>
                <p> Code to reproduce the fine tuning using google colab and google compute engine:
                </p>
                <ul class="actions special">
                    <li><a href="https://github.com/HAJEKEL/CV_LTH_pre-training_depth-estimation/tree/main"
                            class="button large">TRAINING</a></li>
                </ul>
                <p> Code for the pre-training of Resnet-50 on imagenet:
                </p>
                <ul class="actions special">
                    <li><a href="https://github.com/HAJEKEL/CV_LTH_Pre-training" class="button large">DARTS</a></li>
                </ul>
                </p>

            </article>


        </div>

        <!-- Footer -->
                <footer id="footer">

                    <section class="split contact">
                        <section class="alt">
                            <h3>Address</h3>
                            <p>Delft, Netherlands<br />
                        </section>

                        <section>
                            <h3>Email</h3>
                            <p><a>hendrikjekel@gmail.com</a></p>
                        </section>
                        <section>
                            <h3>Social</h3>
                            <ul class="icons">
                                <li><a href="https://www.linkedin.com/in/henk-jekel-748054259/"
                                        class="icon brands alt fa-linkedin"><span class="label">LinkedIn</span></a>
                                </li>
                                <li><a href="https://github.com/HAJEKEL" class="icon brands alt fa-github"><span
                                            class="label">GitHub</span></a></li>
                            </ul>
                        </section>
                    </section>
                </footer>

                <!-- Copyright -->
                <div id="copyright">
                    <ul>
                        <li>Henk Jekel &copy; 2022</li>
                        <li>Design: <a href="https://html5up.net">HTML5 UP</a></li>
                    </ul>
                </div>

        </div>

        <!-- Scripts -->
        <script src="assets/js/jquery.min.js"></script>
        <script src="assets/js/jquery.scrollex.min.js"></script>
        <script src="assets/js/jquery.scrolly.min.js"></script>
        <script src="assets/js/browser.min.js"></script>
        <script src="assets/js/breakpoints.min.js"></script>
        <script src="assets/js/util.js"></script>
        <script src="assets/js/main.js"></script>

</body>

</html>